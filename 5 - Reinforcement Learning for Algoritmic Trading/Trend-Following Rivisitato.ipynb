{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38b1ae68-8c20-49f6-82fe-a63c75b94a25",
   "metadata": {},
   "source": [
    "# RL Trend Following\n",
    "\n",
    "In questo notebook ci limitiamo, in primo luogo, ad usare le API del RL per simulare l'interazione ambiente - agente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da8e1402-a954-4719-bd42-58e362e54ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1f8b9ae-7dbf-49f0-954f-3999b68fa2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"SPY.csv\", index_col=\"Date\", parse_dates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a2225fc-d874-4a48-a235-c330bceafa98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usiamo le finestre che abbiamo già scovato in passato.\n",
    "df[\"FastSMA\"] = df[\"Close\"].rolling(16).mean()\n",
    "df[\"SlowSMA\"] = df[\"Close\"].rolling(33).mean()\n",
    "feats = [\"FastSMA\", \"SlowSMA\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6585028-4691-4488-9200-bf233d2fa58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"LogReturn\"] = np.log(df[\"Close\"]).diff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6220905-40bd-4866-a4a9-4ad703fb39c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_test = 1000\n",
    "train_data = df.iloc[:-N_test].copy()\n",
    "test_data = df.iloc[-N_test:].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a43dd2a-786d-48f3-977e-5ee5fd05b391",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.n = len(df)\n",
    "        self.current_idx = 0\n",
    "        self.action_space = [0, 1, 2] # Buy, Sell, Hold\n",
    "        self.invested = 0\n",
    "\n",
    "        # conversione numpy array più conveniente per indexing\n",
    "        self.states = df[feats].to_numpy()\n",
    "        self.rewards = df[\"LogReturn\"].to_numpy()\n",
    "        self.total_buy_and_hold = 0 # sarò la somma di tutti i reward alla fine dell'episodio seguendo una buy and hold strategy (baseline)\n",
    "\n",
    "    def reset(self):\n",
    "        self.invested = 0\n",
    "        self.current_idx = 0\n",
    "        self.total_buy_and_hold = 0\n",
    "        return self.states[self.current_idx]\n",
    "\n",
    "    def step(self, action):\n",
    "        self.current_idx += 1\n",
    "        if self.current_idx >= self.n:\n",
    "            raise Exception(\"Episode Already Finished\")\n",
    "\n",
    "        if action == 0: # If Buy\n",
    "            self.invested = 1\n",
    "        elif action == 1: # If Sell\n",
    "            self.invested = 0\n",
    "\n",
    "        # Compute Reward\n",
    "        if self.invested:\n",
    "            reward = self.rewards[self.current_idx]\n",
    "        else:\n",
    "            reward = 0\n",
    "\n",
    "        next_state = self.states[self.current_idx]\n",
    "\n",
    "        #baseline\n",
    "        self.total_buy_and_hold += self.rewards[self.current_idx]\n",
    "\n",
    "        # done\n",
    "        done = (self.current_idx == self.n-1)\n",
    "\n",
    "        return next_state, reward, done\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75fafca2-3da8-4ab2-91ad-0a2734d51c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self):\n",
    "        self.is_invested = False\n",
    "\n",
    "    def act(self, state):\n",
    "        assert(len(state)==2)\n",
    "\n",
    "        if state[0] > state[1] and not self.is_invested:\n",
    "            self.is_invested = True\n",
    "            return 0 # Buy\n",
    "\n",
    "        if state[1] > state[0] and self.is_invested:\n",
    "            self.is_invested = False\n",
    "            return 1 # Sell\n",
    "\n",
    "        return 2 # Hold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b6f5502-b149-4de6-84c3-ba1decdeba2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_one_episode(env: Environment, agent: Agent):\n",
    "    \n",
    "    total_reward = 0\n",
    "    done = 0\n",
    "    state = env.reset()\n",
    "    agent.is_invested = False\n",
    "\n",
    "    while not done:\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done = env.step(action)\n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "\n",
    "    print(f\"Reward obtained by Trend Following: {total_reward}. Reward obtained by buy and hold: {env.total_buy_and_hold}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a556b707-e373-41c0-80d3-fdd3a1f466ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward obtained by Trend Following: 0.43459304796456966. Reward obtained by buy and hold: 0.5970866514889401\n"
     ]
    }
   ],
   "source": [
    "env = Environment(train_data)\n",
    "agent = Agent()\n",
    "play_one_episode(env, agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03a59630-fa8c-4140-9b78-fd5b4403afe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward obtained by Trend Following: 0.08889132894199303. Reward obtained by buy and hold: 0.19307543946998518\n"
     ]
    }
   ],
   "source": [
    "env = Environment(test_data)\n",
    "play_one_episode(env, agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d52e7e6-31ad-4754-8163-9e3367b6818f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NB: sembra che adottare una strategia trend following nel buy e seguire una hold, produca più che comprare a caso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f291fd56-80b8-4780-8bd9-8a62985323ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
